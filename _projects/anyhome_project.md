---
layout: page
title: AnyHome
description: Generating your dream home from any description.
img: assets/img/publication_preview/publication_anyhome.png
importance: 3
category: Artificial Intelligence
related_publications: false
---
*The paper is hosted on [arXiv](https://arxiv.org/abs/2312.06644). The source code can be found on [Github](https://github.com/FreddieRao/anyhome_github).*

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/projects/anyhome_teaser.jpg" title="AnyHome Teaser" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <b>Example house-scale indoor scene generated by AnyHome.</b> Users can input any textual description of an indoor scene, and the system is capable of generating house floorplans, room layouts, object placements, and stylistic appearances accordingly. The generated indoor scene is represented by structured and textured room and object meshes. AnyHome enables the synthesis of diverse indoor scenes, allowing users to control scene generation at any stage—from textual input and intermediate representation to the generated meshes.
</div>

The year of 2023 witnessed the booming of Artificial Intelligence Generated Content (AIGC), starting from the outbraek of [ChatGPT](chat.openai.com). The huge potential of ChatGPT in promoting human being's productivity prompted me to embark on a research about the Large Langueg Models. As time progressed, countless of AIGC models were developed, capable of synthesizing content with 1D (text, like ChatGPT) and 2D (images, like Stable Diffusion). However, the 3D content generation was still in its infancy, without a killer model to be widely adopted.

*Will 3D generation be the next huge breakthrough and opportunity in AI?* I kept pondering this question. Indeed, 3D representation had its innate advantages: it aligned with how human beings perceive the world. This representation is more intuitive and realistic, fostering a wide range of applications especially when the concept of MetaVerse became popular.

*Then, Why not catch up with this potential breakthrough?* I urged myself. Immediately, the thought of carrying out a research in 3D generation sparked my interest. My previous research experiences taught me their essence: they were a commitment to steadily accumulate knowledge and insights, enabling me to make meaningful contributions in the future. Researching AI would provide me with the opportunity to explore everything from classic theories to current trends by reading extensive literature, and to settle down and derive complex formulas and create different architectures. It's one thing to understand how Transformers work through self-attention; it's quite another to implement it yourself and see how each component, from positional embedding to skip connections, is indispensable through ablation studies.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/projects/anyhome_result.png" title="AnyHome Results" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <b>Open-Vocabulary Generation Results.</b> AnyHome interprets users’ textual inputs and produces scenes that fulfill the user's free imagination.
</div>

Fortunately, I met [Rao Fu](https://freddierao.github.io/) out of a coincidence during summer 2023, who became both my mentor and "comrade" for the following year till now. We started researching on text-guided, "**egocentric**" indoor scene generation, as we both believed that mimicking the human's "egocentric" perspective would be more intuitive and realistic, just like how we explore our home. As a heavy reader, I always found myself amazed by the vivid description of architecture in novels, whether it was the ancient eastern house from *Dream of the Red Chamber* or the magnificient mansion from *The Great Gatsby*, and I wondered if we could make the dream home in the novel come true. That's why our project was named "**AnyHome**'. 

Every academic research started with literature review: the most tedious yet fruitful period of getting to know a domain. Rao assigned me and my collaborator, **Zichen Liu**, over 30 papers to read over a weekend. We hung out in the cafe, sitting from ten in the morning till eight in the evening, reading and discussing the papers, summarizing each of them and analyzing its pros and cons (you can find an [example](https://hardy-wen.notion.site/Class-2-Scene-Generation-2891c1d14914423ba9c6014534fb7b37) of our note here). It was such a accomplishment looking back at what we've done. I could feel a genuine desire of knowledge between us: we attended the research not for publishing papers but really for learning about the field so that we can catch up with the trend, and therefore we could settle down and get all these inputs without being distracted or feeling impatient.  This revealed to me the importance and benefits of a benign motivation.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/projects/anyhome_pipeline.png" title="AnyHome Method Abstraction" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <b>An abstracted pipeline of our method.</b> You can find the detailed pipeline visualization in our <a href="https://arxiv.org/pdf/2312.06644.pdf">paper</a>.
</div>

We kept up with the hard work, and finally, crafted the pipeline of AnyHome. Taking a free-form textual input, it generates the house-scale scene by: (i) comprehending and elaborating on the user’s textual input through querying an LLM with templated prompts; (ii) converting textual descriptions into structured geometry using intermediate representations like graphs; (iii) employing an SDS process with a differentiable renderer to refine object placements; and (iv) applying depth-conditioned texture inpainting for egocentric texture generation. I won't go into the details here; you can find the complete technical description in our [paper](https://arxiv.org/pdf/2312.06644.pdf).

